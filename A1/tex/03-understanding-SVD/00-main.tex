\section{Primer on Singular Value Decomposition}
\subsection{What is the SVD?}

The following section serves as a primer about \textbf{Singular Value Decomposition} (SVD). All the material in this section will be presented as fact (without proof) and we will ask you to use it to answer the extra credit questions in the sections below. Suppose we have some matrix $\textbf{A} \in \mathbb{R}^{n \times d}$ with rank $r$. Then $\textbf{A}$ has a decomposition given by

\begin{equation}
\textbf{A} = \textbf{U} \textbf{D}\textbf{V}^{T}
\label{svd_decomp}
\end{equation}

which has the following (along with other) properties

\begin{itemize}
    \item $\textbf{U} \in \mathbb{R}^{n \times r}$, $\textbf{D} \in \mathbb{R}^{r \times r}$, $\textbf{V} \in \mathbb{R}^{d \times r}.$
    \item The matrix $\textbf{D}$ is diagonal with positive entries sorted in descending order. We often let let $\sigma_{i} := \textbf{D}_{ii}$, and we then have $\sigma_{1} \geq \sigma_{2} \geq \dots \sigma_{r} > 0$. We call $\sigma_{i}$ the $i$th singular value of the $\textbf{A}$.  
    \item The columns of $\textbf{V}$ form an orthonormal basis for the row space of $\textbf{A}$, and thus $\textbf{V}^{T}\textbf{V} = \textbf{I}$. The $i$th column $\textbf{v}_{i}$ of $\textbf{V}$ is known as the $i$th right singular vector of $\textbf{A}$.
    \item The columns of $\textbf{U}$ form an orthonormal basis for the column space of $\textbf{A}$, and thus $\textbf{U}^{T}\textbf{U} = \textbf{I}$. The $i$th column $\textbf{u}_{i}$ of $\textbf{U}$ is known as the $i$th left singular vector of $\textbf{A}$.
\end{itemize}

The above decomposition is known as the SVD of $\textbf{A}$. 

\subsection{Projections}

Recall that for a linear subspace $W \subset \mathbb{R}^{p}$, the projection of some vector $\textbf{v}$ onto $W$ is defined as
\begin{equation}
\text{proj}_{W}(\textbf{v}) := \argmin_{w \in W} \vert \vert w - \textbf{v}\vert \vert _{2}^2
\label{proj_def}
\end{equation}

In the special case that $W$ is a one dimensional vector space spanned by some $\textbf{w}$, we call this projection the projection of  $\textbf{v}$ onto $\textbf{w}$ which has simple closed form. We give it below, where we have let $\hat{\textbf{w}} := \textbf{w}/\vert \vert \textbf{w}\vert \vert _2$ represent the unit vector pointing in the direction of $\textbf{w}$.

\begin{equation}
\textrm{proj}_{\textbf{w}}(\textbf{v}) = \bigg(\frac{\textbf{v}^{T}\textbf{w}}{\vert \vert \textbf{w}\vert \vert _2^2}\bigg) \textbf{w} = (\textbf{v}^{T}\hat{\textbf{w}}) \hat{\textbf{w}}
\label{proj_1d}
\end{equation}

Intuitively, the quantity $\textbf{v}^{T}\hat{\textbf{w}}$ captures the amount that $\textbf{v}$ points in the direction of $\textbf{w}$.

\subsection{Best Fit Sub-spaces}

Suppose we have a set of vectors $S = \{\textbf{w}_{1}, \dots, \textbf{w}_{n}\}$ where each $\textbf{w}_{i} \in \mathbb{R}^{d}$. We define the $k$-dimensional best-fit subspace for $S$ to be the $k$-dimensional linear subspace $W$ such that

\begin{equation}
W = \argmin_{\textrm{dim}(W) = k} \sum_{i=1}^{n} \vert \vert \textbf{w}_{i} - \textrm{proj}_{W}(\textbf{w}_i)\vert \vert _2^2
\label{best_fit}
\end{equation}

Intuitively, $W$ is the $k$-dimensional subspace that is closest to the vectors in set $S$. \newline

It is an important and interesting fact that, for a matrix $\textbf{A}$ with SVD as given above, the subspace given by $V_{k} = \textrm{span}\{\textbf{v}_{1}, \dots, \textbf{v}_{k}\}$ is the $k$-dimensional best-fit subspace for the rows of $\textbf{A}$. Informally, $\sigma_i$ tells us the degree to which $\textbf{v}_{i}$ contributes to fitting the rows of $\textbf{A}$. Thus the vector which best fits the rows of $A$ is $\textbf{v}_{1}$, the vector which best fits the rows of $\textbf{A}$ when the contribution of $\textbf{v}_1$ is accounted for is $\textbf{v}_2$, etc.


\section{Extra Credit Challenge (5 Points)}
See here for an example of how to submit written assignments - \href{https://bit.ly/3nv3XvP}{https://bit.ly/3nv3XvP}\\~\\

Suppose we have a matrix $\textbf{A} \in \mathbb{R}^{n \times d}$ with SVD $\textbf{A} = \textbf{U} \textbf{D}\textbf{V}^{T}$, where $\textbf{U} \in \mathbb{R}^{n \times r}$, $\textbf{D} \in \mathbb{R}^{r \times r}$, $\textbf{V} \in \mathbb{R}^{d \times r}.$
\begin{enumerate}[(a)]
    \item  \points{4a} Show that  
    \begin{equation}
    \textbf{A} = \sum_{i=1}^{r} \sigma_{i} \textbf{u}_{i} \textbf{v}_{i}^{T} 
    \label{svd_sum}
    \end{equation}
    
    \newpage
    
    \item \points{4b} Show that 
    
    \begin{equation}
    \textbf{u}_i = \frac{1}{\sigma_{i}} \textbf{A}\textbf{v}_{i}
    \label{what_is_u}
    \end{equation}
    
    In particular, the components of $\textbf{u}_i$ represent the size of the projection of the rows of $\textbf{A}$ onto $\textbf{v}_i$ (scaled by $\sigma_{i}$).
    
    \newpage
    
    \item \points{4c} One way of finding a reduced rank approximation of $\textbf{A}$ is by hard-setting all but the $k$ largest $\sigma_i$ to 0. This approximation is called the truncated SVD, and by (a) we see it can be written as 
   
    \begin{equation}
    \textbf{A}_{k} := \sum_{i=1}^{k} \sigma_{i} \textbf{u}_{i} \textbf{v}_{i}^{T}
    \label{reduced_svd_trunc}
    \end{equation}
   
    From (a), we see the truncated SVD can also be written as $\textbf{A}_{k} = \textbf{U}_k \textbf{D}_k \textbf{V}^{T}_k$, where $\textbf{U}_k \in \mathbb{R}^{n \times k}$, $\textbf{V} \in \mathbb{R}^{d \times k}$ are the first $k$ columns of $\textbf{U}, \textbf{V}$, and $\textbf{D} \in \mathbb{R}^{k \times k}$ has the first $k$ singular values.\newline
    
    Show that the rows of $\textbf{A}_{k}$ are the projections of the rows of $\textbf{A}$ onto the subspace of $V_{k}$ spanned by the first $k$ right singular vectors.  \newline
    
    Hint: Recall that the projection of a vector $\textbf{a}$ onto a subspace spanned by $\textbf{v}_1, \dots, \textbf{v}_k$ where the $\textbf{v}_i$ are pairwise orthogonal is given by the sum of projections of $\textbf{a}$ onto the individual $\textbf{v}_i$. 
    
    \newpage
    
    
    \item \points{4d} The Frobenius norm of a matrix $\textbf{M} \in \mathbb{R}^{n \times m}$ is defined as 
    
    \begin{equation}
        \vert \vert \textbf{M}\vert \vert _{F} = \sqrt{\sum_{i=1}^{n} \sum_{j=1}^{m} M_{ij}^2}
        \label{frob}
    \end{equation}
    
    Show that 
    
    \begin{equation}
        \textbf{A}_{k} = \argmin_{\textrm{rank}(\textbf{B}) = k}\vert \vert \textbf{A} - \textbf{B}\vert \vert _{F}
        \label{frob_min}
    \end{equation}

    where the $\argmin$ is taken over matrices of rank $k$. \newline
    
    Hint: Use the fact that $V_{k}$ is the best-fit $k$-dimensional subspace for the rows of $\textbf{A}$.
    
    \newpage
    
\end{enumerate}

\section{Interpretation}

Suppose we have some data matrix $\textbf{X} \in \mathbb{R}^{n \times d}$ with rows $x_i$. The $x_i$ represent $n$ data samples each with $d$ entries. In the case that $d$ is a very large number, we may want to try and reduce the dimensionality of our data. Suppose we want to reduce the dimension of each sample from $d$ to $k < d$. How can we go about doing this? \newline

Consider taking the truncated SVD $\textbf{X}_k$ of our data. From (c) we know that we have transformed our data so all the data samples live within a $k$-dimensional linear subspace, in particular, the $k$-dimensional linear subspace that best-fit our original data. Part (d) tells us that under the Frobenius norm, our new matrix $\textbf{X}_k$ is as close to $\textbf{X}$ as it can be given that it has this property. \newline

Our new approximate data samples (given by the rows of $\textbf{X}_k$) have dimension $d$, but they certainly don't need to. Since each sample lives in the same $k$-dimensional linear subspace, each sample's place in this subspace relative to other samples can be determined by how much it ``points'' in $k$ fixed directions which span the subspace. Thus it suffices to fix $k$ basis vectors for the subspace (these are our $k$ directions) and re-parametrize each sample based on how much it ``points'' in the direction of each of these basis vectors. \newline 

From (c) we know that we can let $\textbf{v}_i, \dots,\textbf{v}_k$ be a basis of our subspace. From (b), we then know that the $i$th row of $\textbf{U}$ denoted $u_i$ is such that its $j$th component $(u_i)_{j}$ tells us how much the $i$th sample points in the direction of $\textbf{v}_j$. Thus, based on the above discussion, we can use the vector $((u_i)_1, \dots, (u_{i})_k)$ as a substitute for the $i$th row of $\textbf{X}_k$. This leaves us with a $k$ dimensional embedding for each of our original $d$ dimensional data samples. The end result is that we use $\textbf{U}_k$ as our new data, and it represents an approximated and transformed $\textbf{X}$.